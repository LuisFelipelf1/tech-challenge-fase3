{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Tech Challenge 3 — Fine‑Tuning FLAN‑T5\n",
        "**Autor:** Luís Felipe Alves — **RM: 363734**  \n",
        "**Entrega individual — FIAP | Pós IA para Devs**  \n",
        "**Última atualização:** 2025-10-02 04:40:31\n",
        "\n",
        "Este notebook realiza o pipeline completo: dados → baseline → **fine‑tuning** → **ROUGE** → `responder()`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0) Instalação e checagens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip -q install -U \"transformers==4.44.2\" \"datasets==2.20.0\" \"sentencepiece==0.2.0\" \\\n",
        "                 \"evaluate==0.4.2\" \"rouge-score==0.1.2\" fsspec gcsfs\n",
        "\n",
        "import sys, torch, random, numpy as np\n",
        "print(\"Python:\", sys.version)\n",
        "print(\"Torch:\", torch.__version__)\n",
        "print(\"CUDA disponível:\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
        "\n",
        "seed=42\n",
        "random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
        "if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Configuração (MODO RÁPIDO/COMPLETO)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import torch\n",
        "\n",
        "MODEL_NAME = \"google/flan-t5-small\"   # pode trocar para 'flan-t5-base'\n",
        "MODO = \"RAPIDO\"   # \"RAPIDO\" ou \"COMPLETO\"\n",
        "\n",
        "if MODO == \"RAPIDO\":\n",
        "    SAMPLE_SIZE = 9000\n",
        "    EPOCHS = 1\n",
        "else:\n",
        "    SAMPLE_SIZE = 30000\n",
        "    EPOCHS = 2\n",
        "\n",
        "BATCH_SIZE = 8\n",
        "GRAD_ACCUM = 4\n",
        "LR = 2e-4\n",
        "MAX_SOURCE_LEN = 96\n",
        "MAX_TARGET_LEN = 192\n",
        "\n",
        "BASE_DIR = Path(\"/content/drive/MyDrive/tc3_flan_t5\")\n",
        "RAW_DIR  = BASE_DIR/\"raw\"; RAW_DIR.mkdir(parents=True, exist_ok=True)\n",
        "TRN_JSON = RAW_DIR/\"trn.json\"  # ou .gz\n",
        "\n",
        "USE_BF16 = torch.cuda.is_available() and (\"a100\" in torch.cuda.get_device_name(0).lower())\n",
        "print(\"MODO:\", MODO, \"| SAMPLE_SIZE:\", SAMPLE_SIZE, \"| EPOCHS:\", EPOCHS, \"| BF16:\", USE_BF16)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Montar Drive e confirmar dados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount(\"/content/drive\")\n",
        "except Exception as e:\n",
        "    print(\"Drive já montado ou indisponível:\", e)\n",
        "\n",
        "print(\"Existe TRN_JSON?\", TRN_JSON.exists())\n",
        "if not TRN_JSON.exists():\n",
        "    print(\"Faça upload de trn.json (ou trn.json.gz) para\", TRN_JSON)\n",
        "    try:\n",
        "        from google.colab import files\n",
        "        up = files.upload()\n",
        "        import shutil\n",
        "        for name in up.keys():\n",
        "            shutil.move(name, TRN_JSON.as_posix())\n",
        "        print(\"Upload concluído:\", TRN_JSON)\n",
        "    except Exception as e:\n",
        "        print(\"Envie manualmente o arquivo para a pasta indicada.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Carregar amostra do dataset (title → content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json, gzip, re\n",
        "\n",
        "def is_gz(p): \n",
        "    try:\n",
        "        with open(p,\"rb\") as f: return f.read(2)==b\"\\x1f\\x8b\"\n",
        "    except: return str(p).endswith(\".gz\")\n",
        "\n",
        "def iter_lines(p):\n",
        "    if is_gz(p): f = gzip.open(p,\"rt\",encoding=\"utf-8\",errors=\"ignore\")\n",
        "    else: f = open(p,\"r\",encoding=\"utf-8\",errors=\"ignore\")\n",
        "    with f:\n",
        "        for line in f:\n",
        "            line=line.strip()\n",
        "            if line: yield line.rstrip(\",\")\n",
        "\n",
        "def norm(s): return re.sub(r\"\\s+\",\" \", (s or \"\").strip())\n",
        "\n",
        "pairs=[]\n",
        "for i, line in enumerate(iter_lines(TRN_JSON)):\n",
        "    if i>=SAMPLE_SIZE: break\n",
        "    try:\n",
        "        obj=json.loads(line)\n",
        "        t,c=norm(obj.get(\"title\",\"\")), norm(obj.get(\"content\",\"\"))\n",
        "        if t and c: pairs.append({\"title\":t, \"content\":c})\n",
        "    except: pass\n",
        "\n",
        "print(\"Amostra carregada:\", len(pairs))\n",
        "print(\"Exemplo:\", pairs[0] if pairs else \"sem dados\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Tokenização (Seq2Seq)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "from datasets import Dataset\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "def make_input(title):\n",
        "    return f\"Generate a concise product description.\\nTitle: {title}\"\n",
        "\n",
        "raw_ds = Dataset.from_list([{\"input\": make_input(p[\"title\"]), \"target\": p[\"content\"]} for p in pairs])\n",
        "raw_ds = raw_ds.train_test_split(test_size=0.06, seed=42)\n",
        "train_raw, val_raw = raw_ds[\"train\"], raw_ds[\"test\"]\n",
        "\n",
        "def preprocess(examples):\n",
        "    model_inputs = tokenizer(examples[\"input\"], max_length=MAX_SOURCE_LEN, truncation=True)\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(examples[\"target\"], max_length=MAX_TARGET_LEN, truncation=True)\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "train_ds = train_raw.map(preprocess, batched=True, remove_columns=train_raw.column_names)\n",
        "val_ds   = val_raw.map(preprocess,   batched=True, remove_columns=val_raw.column_names)\n",
        "print(train_ds, \"\\n\", val_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Baseline (modelo sem ajuste)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSeq2SeqLM\n",
        "import torch\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "base_model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME).to(device).eval()\n",
        "\n",
        "def gerar_base(titles, max_new_tokens=64):\n",
        "    inp = tokenizer([make_input(t) for t in titles], return_tensors=\"pt\",\n",
        "                    padding=True, truncation=True).to(device)\n",
        "    with torch.no_grad():\n",
        "        out = base_model.generate(**inp, max_new_tokens=max_new_tokens)\n",
        "    return tokenizer.batch_decode(out, skip_special_tokens=True)\n",
        "\n",
        "sample_titles = [p[\"title\"] for p in pairs[:3]]\n",
        "print(\"Baseline sample:\", gerar_base(sample_titles)[:1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6) Fine‑tuning (Seq2SeqTrainer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer, AutoModelForSeq2SeqLM\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=MODEL_NAME)\n",
        "\n",
        "args = Seq2SeqTrainingArguments(\n",
        "    output_dir=str(BASE_DIR/\"models\"/\"flan_t5_tc3\"),\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    per_device_eval_batch_size=BATCH_SIZE,\n",
        "    gradient_accumulation_steps=GRAD_ACCUM,\n",
        "    learning_rate=LR,\n",
        "    num_train_epochs=EPOCHS,\n",
        "    logging_steps=50,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    predict_with_generate=True,\n",
        "    bf16=USE_BF16, fp16=not USE_BF16,\n",
        "    report_to=\"none\",\n",
        ")\n",
        "\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME).to(device)\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset=val_ds,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "trainer.save_model(str(BASE_DIR/\"models\"/\"flan_t5_tc3\"))\n",
        "tokenizer.save_pretrained(str(BASE_DIR/\"models\"/\"flan_t5_tc3\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7) Avaliação — ROUGE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import evaluate, numpy as np\n",
        "\n",
        "rouge = evaluate.load(\"rouge\")\n",
        "\n",
        "pred = trainer.predict(val_ds, max_new_tokens=MAX_TARGET_LEN)\n",
        "pred_ids = pred.predictions\n",
        "preds = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
        "\n",
        "labels = pred.label_ids\n",
        "labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "refs = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "results = rouge.compute(predictions=preds, references=refs, use_stemmer=True)\n",
        "print({k: round(v, 4) for k, v in results.items()})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8) Função `responder()` e exemplos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.eval()\n",
        "\n",
        "def responder(title: str, max_new_tokens: int = 128) -> str:\n",
        "    inp = tokenizer([make_input(title)], return_tensors=\"pt\",\n",
        "                    padding=True, truncation=True).to(model.device)\n",
        "    with torch.no_grad():\n",
        "        out = model.generate(**inp, max_new_tokens=max_new_tokens, num_beams=4,\n",
        "                             no_repeat_ngram_size=3)\n",
        "    return tokenizer.decode(out[0], skip_special_tokens=True)\n",
        "\n",
        "for i, t in enumerate(sample_titles, 1):\n",
        "    print(f\"\\n#{i} TITLE:\", t)\n",
        "    print(\"ANTES :\", gerar_base([t])[0][:300])\n",
        "    print(\"DEPOIS:\", responder(t)[:300])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9) Carregar modelo salvo (para gravar sem retreinar)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "LOAD_DIR = str(BASE_DIR/\"models\"/\"flan_t5_tc3\")\n",
        "tok = AutoTokenizer.from_pretrained(LOAD_DIR)\n",
        "mdl = AutoModelForSeq2SeqLM.from_pretrained(LOAD_DIR).to(device).eval()\n",
        "\n",
        "def responder_carregado(title: str, max_new_tokens: int = 128) -> str:\n",
        "    inp = tok([make_input(title)], return_tensors=\"pt\",\n",
        "              padding=True, truncation=True).to(mdl.device)\n",
        "    with torch.no_grad():\n",
        "        out = mdl.generate(**inp, max_new_tokens=max_new_tokens, num_beams=4,\n",
        "                           no_repeat_ngram_size=3)\n",
        "    return tok.decode(out[0], skip_special_tokens=True)\n",
        "\n",
        "print(responder_carregado(sample_titles[0])[:300])"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}